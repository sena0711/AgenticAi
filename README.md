# AgenticAi 
About Agentic AI
2025 04 09 AWS ê´€ë ¨í•˜ì—¬ ì •ë¦¬
"Agent AI"ë€ ìš©ì–´ëŠ” ë¬¸ë§¥ì— ë”°ë¼ ì•½ê°„ì”© ì˜ë¯¸ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œëŠ” ììœ¨ì ìœ¼ë¡œ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„ëœ ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´, ì–´ë–¤ ëª©ì ì´ë‚˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ê³  í–‰ë™í•  ìˆ˜ ìˆëŠ” ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ìë©´:
Agent AIëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë“¤ì„ ê°€ì§ˆ ìˆ˜ ìˆì–´ìš”:
ëª©í‘œ(Goal): ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìì˜ ì´ë©”ì¼ì„ ìë™ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ëŠ” ê²ƒì´ ëª©í‘œì¼ ìˆ˜ ìˆì–´ìš”.
í™˜ê²½ ì¸ì‹(Perception): í˜„ì¬ ì–´ë–¤ ì´ë©”ì¼ì´ ì™”ëŠ”ì§€ë¥¼ ì¸ì‹í•´ì•¼ê² ì£ .
í–‰ë™(Action): ì¤‘ìš” ë©”ì¼ì€ 'ì¤‘ìš”' í´ë”ë¡œ ì˜®ê¸°ê³ , ìŠ¤íŒ¸ì€ ì§€ìš°ëŠ” ì‹ìœ¼ë¡œ í–‰ë™í•©ë‹ˆë‹¤.
ììœ¨ì„±(Autonomy): ì‚¬ëŒì˜ ëª…ë ¹ ì—†ì´ë„ í˜¼ìì„œ ì‘ë™í•  ìˆ˜ ìˆì–´ìš”.

ëŒ€í‘œì ì¸ ì˜ˆ:
OpenAIì˜ AutoGPT, AgentGPT: ì‚¬ìš©ìê°€ "ë‚˜ë¥¼ ìœ„í•œ ì—¬í–‰ ê³„íš ì„¸ì›Œì¤˜"ë¼ê³  í•˜ë©´, ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ìŠ¤ìŠ¤ë¡œ ê³„íší•˜ê³  ì‹¤í–‰í•˜ëŠ” Agent AIì…ë‹ˆë‹¤.

RPA (Robotic Process Automation) + AI: íšŒì‚¬ ì—…ë¬´ ìë™í™”ì— Agent AIë¥¼ ë¶™ì´ë©´, ë‹¨ìˆœí•œ ë°˜ë³µ ì‘ì—…ë¿ ì•„ë‹ˆë¼ íŒë‹¨ê¹Œì§€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ìš”.

ì–´ë””ì— ì“°ì¼ ìˆ˜ ìˆë‚˜ìš”?
ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡
ì£¼ì‹ ë§¤ë§¤ ìë™í™”
ì´ë©”ì¼ ë° ë¬¸ì„œ ìë™ ì •ë¦¬
ì‚¬ì´ë²„ ë³´ì•ˆ ìœ„í˜‘ íƒì§€ ë° ëŒ€ì‘
ê²Œì„ ìºë¦­í„°ì˜ AI í–‰ë™ ì„¤ê³„ ë“±


ğŸ’¡ What is an LLM?
LLM stands for Large Language Model.
Itâ€™s a type of AI model trained on massive amounts of text data to understand and generate human-like language.

âœ… Examples of LLMs:
ChatGPT (by OpenAI)
Claude (Anthropic)
Gemini (Google)
LLaMA (Meta)
Mistral, Command R, etc.

## Reasoning in Agent AI

Reasoning refers to the ability of an Agent AI to make decisions or solve problems based on the information it perceives from its environment. This involves logical thinking, pattern recognition, and decision-making processes that allow the AI to act autonomously.

### Key Aspects of Reasoning:
1. **Logical Deduction**: Drawing conclusions from known facts or rules.
   - Example: If an email is marked as "spam," move it to the spam folder.
   
2. **Pattern Recognition**: Identifying trends or anomalies in data.
   - Example: Detecting unusual login attempts in cybersecurity.

3. **Decision-Making**: Choosing the best action to achieve a goal.
   - Example: Prioritizing important emails over less critical ones.

4. **Learning from Feedback**: Adapting decisions based on past outcomes.
   - Example: Improving email categorization accuracy over time.

Reasoning enables Agent AI to operate effectively in dynamic environments, making it a critical component of autonomy and intelligence.


âœ… Why Auto-Regressive Models Work So Well
1. Natural Language is Sequential
Human language is inherently ordered â€” we understand context based on what came before.

Auto-regressive models mimic this natural flow, which helps them generate coherent, context-aware responses.

2. Great for Generation Tasks
Since they predict one word at a time, AR models are perfect for generating text â€” stories, summaries, code, emails, etc.

Each new word is chosen with awareness of all the words before it, allowing fluid and logical writing.

3. High-Quality Outputs via Transformer Architecture
When combined with Transformer architectures (like in GPT), AR models can:

Attend to long-range dependencies in text

Track complex logic and reasoning

Adapt to many domains: medical, legal, programming...

4. Supports Complex Reasoning
By building answers token by token, they can simulate chains of thought, such as:

"If A is true, then B must be true. Since B leads to C, thenâ€¦"

This "token-by-token logic building" helps in tasks like:

Math

Programming

Step-by-step problem solving

5. Scales Well with Data and Model Size
As AR models get more data and parameters, their ability to learn patterns and generalize improves dramatically (as weâ€™ve seen from GPT-2 â†’ GPT-3 â†’ GPT-4).

ğŸ¤– Contrast: Auto-Encoding Models (e.g., BERT)
BERT-like models use masked language modeling (fill in the blanks).

Great for understanding tasks (classification, QA), but not as good for generating long, coherent text.

Not naturally suited for tasks requiring sequential reasoning or planning.

TL;DR
ğŸ”¥ Auto-regressive models are better for generation and reasoning
Because they:

Predict one token at a time in sequence (like humans speak/write)

Build up logic step-by-step

Scale well with data

Work naturally with Transformers

1. RAG  ê°€ì ¸ì˜¨ ë¬¸ì„œì— ì§ˆë¬¸ì— ì •ë‹µì´ ì—†ì„ë•Œ? -> ì¬ì§ˆë¬¸ ì¬ ë‹µë³€. 
2. Corrective RAG ì§ˆë¬¸ì˜ ë‹µì´ ì €ì¥ì†Œì— ì—†ì„ë•Œ? -> ë‹¤ì‹œë³´ê³  ì›¹ì—ì„œ ê²€ìƒ‰. 

3. Text2SQL - ã…¡muti agent , reflection , tool use  LangGraph+Text2SQL
4. Text2Chart (text to image) - step task decomposer (planning)-> tooluse -> reflection -> retouch prompt.
5. Text2Code - ? 


Bedrock ì— ìˆëŠ” agent capablitiy  
Lang graph ë¡œ use case ê°€ëŠ¥. 

Not first..
1. ê¸°ì¡´ì˜ ì†”ë£¨ì…˜ì˜ ì„±ëŠ¥í–¥ìƒì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ”ê²ƒ. 
2. ì–´ë µê²Œ í•œë‹¤ê³ í•´ì„œ ì˜í•œë‹¤ê³  ìƒê°í•˜ì§€ ì•ŠëŠ”ë‹¤. 
3. ê°„ë‹¨í•˜ê²Œ. 
4. êµ¬ë¶„í•˜ì§€ ë§ê³  ë¬¸ì œë¥¼ í’€ì. 

''
## Reasoning in Agent AI

Bedlock langchain. 




âœ… Amazon Bedrockê³¼ LangChainì˜ ê´€ê³„
í•­ëª©	ì„¤ëª…
Amazon Bedrock	AWSì—ì„œ ì œê³µí•˜ëŠ” ë©€í‹° LLM ì„œë¹„ìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤. ì—¬ëŸ¬ LLM (Anthropic Claude, Meta LLaMA, Mistral, Amazon Titan ë“±)ì„ APIë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” í˜¸ìŠ¤íŒ… ì¸í”„ë¼ì…ë‹ˆë‹¤.
LangChain	LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ê¸° ìœ„í•œ Python/JS ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì—¬ëŸ¬ LLM, ë„êµ¬, ì²´ì¸, í”„ë¡¬í”„íŠ¸ ë“±ì„ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.
ğŸ¤ ê´€ê³„
LangChainì€ Bedrockì„ LLM í”„ë¡œë°”ì´ë” ì¤‘ í•˜ë‚˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¦‰, LangChain ì•ˆì—ì„œ Bedrock ëª¨ë¸ì„ ì“¸ ìˆ˜ ìˆëŠ” ê²ƒì´ì§€, Bedrock ì•ˆì— LangChainì´ ë‚´ì¥ëœ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.

ğŸ”§ ì˜ˆì‹œ êµ¬ì¡°
plaintext
Copy
Edit
[Your LangChain App]
        |
        v
[LLM Wrapper (LangChain)]
        |
        v
[Amazon Bedrock API]
        |
        v
[Claude / Titan / LLaMA ë“±]
LangChainì˜ LLM wrapperì—ì„œ Bedrockì„ backendë¡œ ì„¤ì •í•˜ë©´ Amazon LLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì˜ˆìš”.




ì„¤ê³„. 


tool use, tavily ì¸í„°ë„· ê²€ìƒ‰, ë„ì„œ ì •ë³´ ì¡°íšŒ (ì™¸ë¶€ API ) , ë‚ ì”¨ ì •ë³´ ì¡°íšŒ (ì™¸ë¶€ api) , Code Interpreter ì‚¬ìš©í•˜ê¸° , Code Drawer..

Riza - sandbox...ì•ˆì „í•˜ê²Œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ”ì¼ì„ ìˆ˜í–‰í• ìˆ˜ ìˆìŒ. 

ì½”ë“œëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ LLM 

build chat agent with history. 
checkpoint= checkpointer store= memorystore 
ëŠë ¤ì§


semanitic memory
episodic memory
procedual memory. 

ë‚´ì´ë¦„ì€ ê²½ìˆ˜ì•¼, ë‚´ì´ë¦„ ê¸°ì–µí•˜ë‹ˆ?

tread_id : userid... 



## Builder. 
Crag -  - retrieve, grade, generate. 

[text](https://build.langchain.com/)



Crag 
ToolUse
Planning
Reflection 